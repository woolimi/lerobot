# 7일 집중 플랜 (붕어빵 인형 바구니 대회)

**대회일**: 2월 7일 · **로봇**: SO-ARM101 · **FPS**: 25 (카메라 제한으로 고정)

녹화·훈련·시연 모두 **25fps**로 통일. (`--dataset.fps=25`)

---

## 💡 일주일간 명심할 원칙

1. **쓰레기 데이터를 넣지 마세요 (GIGO)**  
   조종하다 실수했다면, 그 에피소드는 **삭제하고 다시 녹화**하세요.

2. **복구 동작을 포함하세요**  
   인형을 집다가 살짝 미끄러졌을 때, **다시 자세를 고쳐 잡고 성공**시키는 장면을 **5~10개** 꼭 녹화하세요. 우승을 좌우하는 **지능**이 됩니다.

3. **팔을 부드럽게 움직이세요**  
   로봇 조종 시 **일정한 속도**와 **부드러운 곡선**으로 움직이세요. 갑자기 끊거나 튀는 동작은 학습 품질을 떨어뜨립니다.

---

## 훈련 방식 (권장) · 모델 버전명

**한 번에 훈련**으로 진행합니다. 중간·최종 모델에 아래 버전명을 붙이면 관리하기 쉽습니다.

| 시점 | 데이터 | 버전명 | 비고 |
|------|--------|--------|------|
| D-7 | 10개 | **v0.1-pilot** | 방향만 확인, 본격 학습 아님 |
| D-6 끝 (선택) | 80개 | **v0.2-check** | 데이터·방향 점검용, 이어 쓰지 않음 |
| D-4 | 150개 | **v1.0** | 본 학습, 대회용 후보 |
| D-3 | 150개 + 보강 | **v1.1** | 보강 반영, 최종 후보 |

- **D-7**: 10개로 짧게 학습 → **v0.1-pilot** (방향만 확인).
- **D-6 끝**: 여유 있으면 80개로 1~2시간만 → **v0.2-check** (점검용, D-4에서 처음부터 학습).
- **D-4**: 150개 전체로 한 번에 학습 → **v1.0** (MPS, Jittering).
- **D-3**: 150개 + 보강으로 한 번에 다시 학습 → **v1.1** (최종 후보).

### 훈련 시 줄 옵션 (공통)

| 옵션 | 권장/예시 | 설명 |
|------|-----------|------|
| `--policy.device` | `mps` (Mac) / `cuda` (NVIDIA) | 학습·추론 디바이스 |
| `--batch_size` | `32` (메모리 허용 범위까지) | 배치 크기. M4 36GB면 32~64 시도 |
| `--steps` | D-7: `5000` / D-4: `100000` 이상 | 학습 스텝 수. 데이터 많으면 늘리기 |
| `--dataset.image_transforms.enable` | `true` | Color Jittering (조명 강인함) |
| `--output_dir` | `outputs/train/act_bun_basket` | 체크포인트 저장 경로 |
| `--job_name` | `act_bun_basket` | 실험 이름 (로그·폴더에 사용) |
| `--wandb.enable` | `true` (선택) | Loss 등 모니터링용 |

**예시 (D-4 본 학습)**  
`batch_size=32`, `steps=100000` (또는 150000), `device=mps`, Jittering 켜기.  
**예시 (D-7 짧게)**  
`batch_size=8` 또는 `16`, `steps=5000`, 방향만 확인.

---

## D-7 (오늘): 환경 고정 및 샘플 테스트

- [x] 붕어빵 5개 배치할 시작 위치 3~5곳 표시
- [x] Top-down 카메라 장착·각도 조정 (바닥·인형·바구니가 한 화면에)
- [x] Wrist 카메라 장착·각도 조정 (그립 시 인형이 잘 보이도록)
- [x] 5개 깔고 1개 담기 방식으로 10개 에피소드 녹화
- [x] 10개 데이터로 ACT 짧게 학습 → **v0.1-pilot** 저장  
  `./scripts/04.train_act_pilot.sh`
- [x] 로봇이 인형 쪽으로 팔을 뻗는지 확인 (방향성만 확인)

---

## D-6: 메인 데이터 수집 1일차 (80개)

- [x] 5개 깔고 하기 20회
- [x] 4개 깔고 하기 20회
- [x] 3개 깔고 하기 20회
- [x] 2개 깔고 하기 20회
- [x] 당일 녹화분 에피소드 수 확인 (총 80개)
- [ ] *(선택)* 80개로 1~2시간 짧게 학습 → **v0.2-check** 저장 (점검용, D-4에서 처음부터 학습)  
  `MODEL_VERSION=v0.2-check ./scripts/04.train_act_first_try.sh --steps=10000 --batch_size=16`  
  (데이터 경로 바뀌면 `DATASET_ROOT=./data/...` 추가)

---

## D-5: 메인 데이터 수집 2일차 (70개)

- [ ] 1개 깔고 하기 40회
- [ ] 인형 겹쳐 있는 배치 15회 정도
- [ ] 구석·가장자리 배치 15회 정도
- [ ] 복구 동작(미끄러졌다가 다시 잡기) 에피소드 5~10개 포함
- [ ] D-6 + D-5 합쳐 **150개** 에피소드 확인 (5·4·3·2·1 모두 포함, 많을수록 유리)

---

## D-4: 집중 학습 (한 번에 훈련)

- [ ] 수집한 **150개 전체**로 ACT **한 번에** 학습 (처음부터) → **v1.0** 저장  
  `MODEL_VERSION=v1.0 DATASET_ROOT=./data/bun_basket ./scripts/04.train_act_first_try.sh --batch_size=32 --steps=100000`
- [ ] Loss 안정적으로 감소하는지 확인
- [ ] 성적 가장 좋은 체크포인트 확보 (v1.0)

---

## D-3: 1차 실전 테스트 및 데이터 보강

- [ ] 학습된 모델로 실기 테스트
- [ ] 잘 되는 경우 / 자주 실수하는 경우 메모
- [ ] 실수하는 상황만 20~30개 추가 녹화
- [ ] 기존 150개 + 보강 데이터로 **한 번에** 다시 학습 (처음부터) → **v1.1** 저장  
  `MODEL_VERSION=v1.1 DATASET_ROOT=./data/bun_basket ./scripts/04.train_act_first_try.sh --batch_size=32 --steps=100000`
- [ ] 새 best 체크포인트 확보 (v1.1)

---

## D-2: 실전 시뮬레이션 및 변수 차단

- [ ] 5개 연속 투입 연습 (대회와 동일 조건)
- [ ] 방 등 끄기 / 스탠드만 켜기 등 조명 변경 후 테스트
- [ ] v1.0 vs v1.1 중 가장 안정적인 **best_model** 선별
- [ ] 선별한 버전을 Hugging Face Private Repo 및 USB에 백업

---

## D-1: 컨디션 조절 및 장비 점검

- [ ] 로봇 팔 나사 풀린 곳 확인
- [ ] 케이블 연결 헐거움 확인
- [ ] 카메라·바구니·시트지 위치 그대로인지 확인
- [ ] 5개 연속 투입 5세트 리허설
- [ ] 성공률 90% 이상이면 학습 종료
- [ ] 노트북 충전기·마우스·USB(백업)·케이블 챙기기

---

*전략 상세: [competition_strategy_bun_basket.md](competition_strategy_bun_basket.md)*
